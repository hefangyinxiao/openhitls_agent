import os

from modelscope import snapshot_download
from transformers import AutoModel, AutoTokenizer

# é…ç½®
MODELS = {
    "embedding": "qwen/Qwen3-Embedding-0.6B",  # ModelScopeçš„æ¨¡å‹ID
    "reranker": "qwen/Qwen3-Reranker-0.6B"
}
CACHE_DIR = "./workspace/model_cache"
os.makedirs(CACHE_DIR, exist_ok=True)


def download_with_modelscope(model_name: str):
    try:
        print(f"â¬‡ï¸ æ­£åœ¨é€šè¿‡ModelScopeä¸‹è½½ {model_name}...")

        # å…³é”®ä¸‹è½½å‚æ•°
        model_dir = snapshot_download(
            model_id=model_name,
            cache_dir=CACHE_DIR,
            revision='master',  # ä½¿ç”¨ä¸»åˆ†æ”¯
            ignore_file_pattern=[".*.md", "*.bin"],  # è¿‡æ»¤éå¿…è¦æ–‡ä»¶
        )
        print(f"âœ… ä¸‹è½½å®Œæˆï¼æ¨¡å‹ä¿å­˜åˆ°: {model_dir}")
        return model_dir
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {str(e)}")
        return None


if __name__ == "__main__":
    for model_type, model_name in MODELS.items():
        model_path = download_with_modelscope(model_name)

        # éªŒè¯æ¨¡å‹
        if model_path:
            try:
                print("ğŸ§ª éªŒè¯æ¨¡å‹åŠ è½½...")
                if "Embedding" in model_name:
                    from langchain_huggingface import HuggingFaceEmbeddings

                    embedding = HuggingFaceEmbeddings(
                        model_name=model_path,
                        model_kwargs={"device": "cpu"}
                    )
                    print(f"åµŒå…¥æ¨¡å‹æµ‹è¯•ç»“æœ: {len(embedding.embed_documents(['æµ‹è¯•'])[0])}ç»´")
                else:
                    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
                    model = AutoModel.from_pretrained(model_path, trust_remote_code=True)
                    print(f"é‡æ’åºæ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
            except Exception as e:
                print(f"âš ï¸ æ¨¡å‹éªŒè¯å¤±è´¥: {str(e)}")